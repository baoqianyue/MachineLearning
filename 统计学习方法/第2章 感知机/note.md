# 感知机(Perceptron)     

感知机是面向二分类的线性分类模型，输入为样本实例的特征向量，输出只有两个值,-1和1，表示输入实例的类别。        

在几何意义上来看，感知机对应于在输入空间(特征空间)将实例划分为正负两类的一个超平面,为了找到最佳的划分平面，需要导入错误分类的损失函数，训练使该损失函数极小化，就可以求得感知机模型。    

感知机模型分为原始形式和对偶形式，1957年由Rosenblatt提出，感知机模型是神经网络和支持向量机的基础模型。     

## 感知机模型    

假设输入x为样本的特征向量，对应于特征空间中的一点，输出y表示该样本的类别，感知机就是从输入x到输出y的一个函数：   

$$f(x) = sign(\omega{x} + b)$$     

其中`ω`和`x`是感知机模型的参数，`ω`是权值(weight)或者权值向量(weight vector),`b`是偏置项(bias)     

`sign`是符号函数：   

$$
sign(x) = \left\{\begin{aligned}
+1 & & x \ge 0 \\  
-1 & & x < 0
\end{aligned}
\right. 
$$      

感知机模型的假设空间是由定义在特征空间中的所有线性分类模型构成的，所以线性划分方程：   

$$\omega{x} + b = 0$$     

对应于特征空间中的一个超平面，其中`w`是超平面的法向量，`b`是超平面的截距，将特征空间划分为两个区域。   

## 损失函数   

### 数据集的线性可分性    

上面提到感知机模型可以看作是特征空间中的一个划分平面，所以这里首先来介绍一下数据集的线性可分性     

给定一个数据集:   

$$T = {(x_1, y_1), (x_2, y_2),...,(x_N, y_N)}$$   

其中`xi`位于该数据集构成的特征空间中，`y={-1, +1},i = 1,2,...,N`，如果存在某个超平面S可以将该数据集中所有的正类点和负类点完全正确的划分到超平面的两侧，就称为该数据集T为线性可分数据集(linearly separable data set).   


### 学习策略   

为了找到合适的超平面，首先要导入一个能够描述错误分类的函数，而且这个函数需要是模型参数`w和b`的连续可导函数，方便优化。    

这里使用误分类点到超平面S的总距离来构造损失函数  

输入空间中任一点x0到超平面S的距离为：  

$$\frac{1}{||\omega||}|\omega{x_0} + b|$$   

这里`||ω||`是`ω`的`L2`范数       

光凭一个距离还是无法较好的表征分类预测的确信程度，所以这里引入了函数间隔(functional margin)  

* 函数间隔    

    对于一组样本来说，如果分类超平面`ωx+b=0`已经确定，`|ωx+b|`能够表示点x距离超平面的远近，而`ωx+b`的符号与类标记的y的符号是否一致能够表示分类是否正确。   

    所以可以使用`y(ωx+b)`来表示分类的正确性及确信度，这就是函数间隔(functional margin)  

这里考虑对错误分类的数据点，当`ωx+b>0`时，因为是错误分类，所以`yi=-1`，而当`ωx+b<0`时，`yi=+1`,所以错误分类点`xi`到超平面的距离是：  

$$-\frac{1}{||\omega||}y_i(\omega{x_i} + b)$$   

假设错误分类的点集合为M，那所有错误分类点到超平面S的总距离为：   

$$-\frac{1}{||\omega||}\sum_{x_i\in{M}}y_i(\omega{x_i} + b)$$    

然后得到感知机学习的损失函数:   

$$L(\omega,b) = -\sum_{x_i \in {M}}y_i(\omega{x_i} + b)$$   

这个损失函数就是感知机学习经验风险函数    


