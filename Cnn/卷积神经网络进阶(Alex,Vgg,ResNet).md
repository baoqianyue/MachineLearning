## 卷积神经网络的演变    

AlexNet、VGG、ResNet、Inception、MobileNet   

## AlexNet(2012)        

* 网络结构    

    ![img](./imgs/alexnet.png)     

    * 输入为224x224，3通道   
    * 第一层卷积为11x11的卷积，共有48x2个卷积核，stride=4    
    * 第二层卷积为5x5的卷积，共有128x2个卷积    
    * 第二层max_pooling为2x2，(55/27)
    * 在第三层卷积层后，进行两个GPU上输出神经元图的交叉       
    ...      

    * 首次使用ReLU激活函数       
    * 第一层全连接：4096    
    * 第二层全连接：4096  
    * 两个全连接层都使用了dropout      
    * Softmax：1000，计算概率值        
    * dropout = 0.5    
    * batch size = 128   
    * learning_rate = 0.01,过一定的次数降低为1/10    

* 输出神经元图size的计算   

    对第一个卷积层输出计算   

    * 输入大小为224x224    
    * Stride=4,卷积核size为11x11   
    * 输出大小=(输入大小-卷积核size+padding)/stride + 1 = 55  
    *         (224 - 11 + 3) / 4 + 1 = 55    
    * 参数数量: 3 x (11 x 11) x 96 = 35k     


* Dropout      

    在进行fc层连接时，随机的给上层神经元加上一个mask，相当于随机减少参与连接的神经元数量        

    但是在每次训练迭代时，在反向传播计算时，这个随机的mask都会发生变化    
    
    ![dp](./imgs/dropout.png)    

    * 为什么运用在全连接层上     
        * 全连接层参数占全部参数的大部分，容易发生过拟合   
        * 每次dropout操作都相当于训练了一个子网络，最后的结果相当于很多子网络的组合     
        * 过拟合可以理解为模型记住了数据集的分布，dropout操作消除了神经元之间的依赖关系，增强了模型的泛化能力     
        * 对于dropout操作后的结果，总能找到一个样本与这个结果对应，相当于引入了新的数据(数据增强)    

## VGGNet(2014)      

该网络被提出的目的是为了探究在大规模图像识别任务中，卷积网络深度对模型精确度到底有何影响。     

* 网络结构      

    * 层次更深   
    * 使用2个串联3x3的卷积核代替5x5卷积核     
    * 使用3个串联3x3的卷积核代替7x7卷积核         
    * 多使用1x1的卷积核    
        1x1的卷积层可以看作是非线性变换    
    * 每经过一个pooling层，通道数目翻倍    
        因为经过pooling后，图像尺寸会减小，会丢失信息，所以提高通道数量，对应提取的特征数量会增加，减小信息丢失的程度     

    ![vgg](./imgs/vgg2.png)     

* 视野域     

    ![vgg1](./imgs/vgg1.png)     

    这里可以看作两个3x3的串联卷积得到的效果等同于一个5x5的卷积     

    使用两个串联3x3卷积有什么好处呢    

    * 2层卷积比1层多一次非线性变换    
        多一次非线性变换会影响到最后的拟合效果         
    * 参数数量降低28%     
        局部连接的神经元数量会减少       

* 1x1的卷积层等价于非线性变换     

    使用多通道的1x1的卷积核对输入图像进行操作，当3个卷积核对对应3个通道上的像素值计算完成后,直接图像中对应坐标上3个通道对应卷积运算的结果进行叠加就得到了输出神经元的值，**这个过程可以看作是在通道这个维度上进行的全连接(非线性变换)**      

* LRN       

    归一化操作，LRN考虑的是在同一层次(卷积层或者全连接层，拥有多个通道)中的相邻像素进行归一化操作(均值为0，方差为1)    

    这个方法在AlexNet中有很大的优化作用，但是在其他网络中并没有很大的贡献，而且已经被BatchNormalization和Dropout操作所取代      
## ResNet      

* 网络层次加深的问题    

    ![deeper problem](./imgs/resnet1.png)  

    图中可以看出随着训练迭代次数的增加，56层的网络在训练集上的准确率反而没有20层的网络高，这表明网络层次深度到达一定程度时，训练效果会到达一个瓶颈    

    首先有这样一个假设：深层网络更难优化并非深层网络学不到东西    

* 残差结构   

    ![res](./imgs/resnet2.png)     

    这个残差单元中，x为原输入，左边为卷积分支，有两个卷积层，右侧为shortcut connections分支，这个分支相当于恒等变换。     

    设原本经过两个卷积层后输出的神经元图为H(x)        

    而两个卷积层的作用为F(x)        

    那得到：   

    $$H(x) = x + F(x)$$     

    这里F(x)被称为残差，当F(x)趋于0时，表明这两个卷积层的作用为恒等变换，与右侧的shortcut connection分支作用一样    

    如果没有这个短路分支的话，这里学习到的特征为`F(x)+x`，**之所以这样这样是因为残差学习相比原始特征直接学习更容易（梯度传递更容易和充分）**,当残差为0时，此时卷积层仅仅做了恒定映射，至少网络性能是不会下降的，实际上残差也不会为0，这也使得卷积层在输入特征基础上学习到了新的特征，从而提高了网络的性能。    

    * 为什么残差学习相对容易：  
        首先直观上残差的值会很小，所以需要学习的内容比较少，所以学习难度会很小。   

        从数学角度分析一下：  

        ![img](./imgs/resblock.png)     

        上式第一个因子表示损失函数对L的梯度，括号中的1代表短路机制可以无损的传播梯度，而另外一项残差梯度需要经过带有weights的卷积层，想比短路通路，梯度不是直接传递过来的，而且残差梯度不会都是-1，与短路通路的梯度抵消，就算很小，也不会导致梯度消失，所以残差学习会更加容易。    



        
## 参考   

[你必须要知道CNN模型：ResNet](https://zhuanlan.zhihu.com/p/31852747)   


       




    




