## 卷积神经网络的演变    

AlexNet、VGG、ResNet、Inception、MobileNet   

## AlexNet(2012)        

* 网络结构    

    ![img](./imgs/alexnet.png)     

    * 输入为224x224，3通道   
    * 第一层卷积为11x11的卷积，共有48x2个卷积核，stride=4    
    * 第二层卷积为5x5的卷积，共有128x2个卷积    
    * 第二层max_pooling为2x2，(55/27)
    * 在第三层卷积层后，进行两个GPU上输出神经元图的交叉       
    ...      

    * 首次使用ReLU激活函数       
    * 第一层全连接：4096    
    * 第二层全连接：4096  
    * 两个全连接层都使用了dropout      
    * Softmax：1000，计算概率值        
    * dropout = 0.5    
    * batch size = 128   
    * learning_rate = 0.01,过一定的次数降低为1/10    

* 输出神经元图size的计算   

    对第一个卷积层输出计算   

    * 输入大小为224x224    
    * Stride=4,卷积核size为11x11   
    * 输出大小=(输入大小-卷积核size+padding)/stride + 1 = 55  
    *         (224 - 11 + 3) / 4 + 1 = 55    
    * 参数数量: 3 x (11 x 11) x 96 = 35k     


* Dropout      

    在进行fc层连接时，随机的给上层神经元加上一个mask，相当于随机减少参与连接的神经元数量        

    但是在每次训练迭代时，在反向传播计算时，这个随机的mask都会发生变化    
    
    ![dp](./imgs/dropout.png)    

    * 为什么运用在全连接层上     
        * 全连接层参数占全部参数的大部分，容易发生过拟合   
        * 每次dropout操作都相当于训练了一个子网络，最后的结果相当于很多子网络的组合     
        * 过拟合可以理解为模型记住了数据集的分布，dropout操作消除了神经元之间的依赖关系，增强了模型的泛化能力     
        * 对于dropout操作后的结果，总能找到一个样本与这个结果对应，相当于引入了新的数据(数据增强)    

## VggNet(2014)    


