## 卷积神经网络(Cnn)    

### 神经网络遇到的问题   

* 参数过多    

    举例：     

    * 图像大小为`1000*1000`
    * 下一层神经元为`10^6`     
    * 全连接参数为`1000*1000*10^6=10^12`      

    * 容易过拟合，需要更多的训练数据     
    * 收敛到较差的局部极值    

* 卷积———解决问题      

    局部连接      

    **图像特征是以区域为单位的**      

    ![conv2](./imgs/conv.jpg)     

    下一层神经元中的每个只和图像中的一个区域相连，这个区域是`10*10`的，相当于100个像素      

    * 图像大小为`1000*1000`    
    * 下一层神经元为`10^6`    
    * 局部连接范围为`10*10`     
    * 全连接参数为`10*10*10^6=10^8`     

    参数共享       

    **图像的特征和像素的位置是无关的**，如果让每一个神经元去学习一个固定的像素的话，学习到的特征是无意义的，因为像素的位置是会变化的。     

    强制每一个神经元的局部连接都使用同样的参数   

    参数共享对减少参数过多问题的效果     

    * 图像大小为`1000*1000`
    * 下一层神经元为`10^6`     
    * 局部连接范围为`10*10`   
    * 全连接参数为`10*10=100`          


### 卷积过程的输入输出关系    

`输出size = 输入size - 卷积核size + 1`    

* 步长stride     

* padding使输出size不变    



### 池化    

* 最大值池化(Max Pooling)   

* 均值池化     

* 常使用不重叠，不补零(池化核大小等于步长)     

* 没有用于求导的参数(核可以看为是一个空核)     

* 池化层的参数为步长和核大小   

* 用于减少图像尺寸，从而减少计算量    

* 一定程度上解决了平移鲁棒性     

* 损失了空间位置精度(如果使用最大池化，会丢失一些图像细节信息)    

* 池化可以看作是计算量和图像精度的交换   

* 输出尺寸 = n / kernel size

### 全连接层    

* 即普通神经网络的层     

* 将上一层输出展开并连接到每一个神经元上    

    卷积层和池化层的输出都是二维矩阵形式，为了与普通层上的神经元相连，就需要将输入展开成向量形式    

    这里需要注意，一旦图像被展开，就再也无法加入卷积层或池化层了     

* 相比于卷积层，参数数目较大     

* 参数数目 = Ci * Co      

    * Ci，Co为输入输出通道数目    


### 卷积神经网络结构       

* 卷积层+池化层+全连接层    

    ![cnn](./imgs/cnnst.png)    

    这种结构最后的全连接层输出的是一个向量    

    如果将全连接层去掉，就可以输入图像，输出图像      

* 卷积层+池化层     

    ![cnnout](./imgs/cnnout.png)    

    这里可以在网络的最后加入一些反卷积层，来将卷积或池化的较小图像结果生成到与输入图像相同的尺寸，就可以完成一些图像的分割任务     

    








