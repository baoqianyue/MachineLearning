{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.0+cu100'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先定义一些超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "EPOCHS = 20 # 对数据集迭代20次\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # 自动判断当前是否支持gpu环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接通过DataLoader下载数据并读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 9854976/9912422 [00:15<00:00, 1084252.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/28881 [00:00<?, ?it/s]\u001b[A\n",
      "32768it [00:00, 35608.47it/s]                           \u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1648877 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 16384/1648877 [00:00<00:29, 55874.22it/s]\u001b[A\n",
      "  2%|▏         | 40960/1648877 [00:01<00:25, 63607.67it/s]\u001b[A\n",
      "  6%|▌         | 98304/1648877 [00:01<00:19, 80823.09it/s]\u001b[A\n",
      "  9%|▉         | 155648/1648877 [00:01<00:14, 99747.44it/s]\u001b[A\n",
      " 13%|█▎        | 221184/1648877 [00:01<00:11, 121466.43it/s]\u001b[A\n",
      " 17%|█▋        | 278528/1648877 [00:02<00:09, 140783.98it/s]\u001b[A\n",
      " 21%|██        | 344064/1648877 [00:02<00:08, 162194.47it/s]\u001b[A\n",
      " 25%|██▍       | 409600/1648877 [00:02<00:06, 205841.62it/s]\u001b[A\n",
      " 27%|██▋       | 442368/1648877 [00:02<00:05, 208244.84it/s]\u001b[A\n",
      " 29%|██▉       | 483328/1648877 [00:02<00:04, 240958.24it/s]\u001b[A\n",
      " 31%|███▏      | 516096/1648877 [00:02<00:04, 230950.26it/s]\u001b[A\n",
      " 34%|███▍      | 557056/1648877 [00:03<00:04, 263086.65it/s]\u001b[A\n",
      " 36%|███▌      | 589824/1648877 [00:03<00:04, 245892.40it/s]\u001b[A\n",
      " 39%|███▉      | 638976/1648877 [00:03<00:03, 266424.27it/s]\u001b[A\n",
      " 41%|████      | 671744/1648877 [00:03<00:03, 271361.66it/s]\u001b[A\n",
      " 44%|████▎     | 720896/1648877 [00:03<00:02, 311207.23it/s]\u001b[A\n",
      " 46%|████▌     | 761856/1648877 [00:03<00:03, 291458.45it/s]\u001b[A\n",
      " 49%|████▉     | 811008/1648877 [00:03<00:02, 306575.94it/s]\u001b[A\n",
      " 52%|█████▏    | 851968/1648877 [00:03<00:02, 316379.14it/s]\u001b[A\n",
      " 55%|█████▌    | 909312/1648877 [00:04<00:02, 286909.09it/s]\u001b[A\n",
      " 61%|██████    | 999424/1648877 [00:04<00:01, 337099.51it/s]\u001b[A\n",
      " 63%|██████▎   | 1040384/1648877 [00:04<00:01, 338183.44it/s]\u001b[A\n",
      " 67%|██████▋   | 1105920/1648877 [00:04<00:01, 314151.97it/s]\u001b[A\n",
      " 73%|███████▎  | 1204224/1648877 [00:04<00:01, 368609.12it/s]\u001b[A\n",
      " 76%|███████▌  | 1253376/1648877 [00:05<00:01, 378141.37it/s]\u001b[A\n",
      " 80%|███████▉  | 1318912/1648877 [00:05<00:00, 400428.70it/s]\u001b[A\n",
      " 83%|████████▎ | 1368064/1648877 [00:05<00:00, 403624.56it/s]\u001b[A\n",
      " 87%|████████▋ | 1433600/1648877 [00:05<00:00, 422084.00it/s]\u001b[A\n",
      " 90%|████████▉ | 1482752/1648877 [00:05<00:00, 422270.97it/s]\u001b[A\n",
      " 94%|█████████▍| 1556480/1648877 [00:05<00:00, 445376.13it/s]\u001b[A\n",
      " 97%|█████████▋| 1605632/1648877 [00:05<00:00, 441305.91it/s]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "8192it [00:00, 14848.23it/s]            \u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "9920512it [00:29, 1084252.57it/s]                             \n",
      "1654784it [00:23, 441305.91it/s]                             \u001b[A"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True,\n",
    "                  transform=transforms.Compose([\n",
    "                      transforms.ToTensor(),\n",
    "                      transforms.Normalize((0.1307,), (0.3081,))\n",
    "                  ])),\n",
    "    batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个卷积神经网络   \n",
    "网络包含两个卷积层，conv1和conv2，紧接着两个fc层作为输出    \n",
    "定义输出层为10个维度，根据这个10维度的输出来确定识别的数字结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 输入数据维度(batch_size, 1, 28, 28)\n",
    "        # Conv1d参数：输入通道数，输出通道数，卷积核size\n",
    "        self.conv1 = nn.Conv2d(1, 10, 5)\n",
    "        # output: [10]\n",
    "        self.conv2 = nn.Conv2d(10, 20, 3)\n",
    "        # output: [20]\n",
    "        # fc层参数: 输入通道数，输出通道数\n",
    "        self.fc1 = nn.Linear(2000, 500)\n",
    "        # output: [500]\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "        # output: [10]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0) \n",
    "        # [512, 1, 28, 28]\n",
    "        out = self.conv1(x)\n",
    "        # output: [bs, 10, 24, 24]\n",
    "        out = F.relu(out)\n",
    "        out = F.max_pool2d(out, 2, 2)\n",
    "        # output: [bs, 10, 12, 12]\n",
    "        out = self.conv2(out)\n",
    "        # output: [bs, 20, 10, 10]\n",
    "        out = F.relu(out)\n",
    "        # flatten\n",
    "        out = out.view(in_size, -1) \n",
    "        # output: [bs, 20*10*10=2000]\n",
    "        out = self.fc1(out)\n",
    "        # output: [bs, 500]\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        # output: [bs, 10]\n",
    "        out = F.log_softmax(out, dim=1) \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例化上面定义的神经网络    \n",
    "使用to方法将网络移动到GPU容器    \n",
    "优化器使用adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义训练脚本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimiter, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # 梯度清零\n",
    "        optimiter.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # bp梯度\n",
    "        loss.backward()\n",
    "        # 更新参数\n",
    "        optimiter.step() \n",
    "        if(batch_idx + 1) % 30 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f})%] \\t Loss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义测试脚本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            # 找到概率最大的index\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%) \\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset), \n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [14848/60000 (25)%] \t Loss: 0.117646\n",
      "Train Epoch: 0 [30208/60000 (50)%] \t Loss: 0.109217\n",
      "Train Epoch: 0 [45568/60000 (75)%] \t Loss: 0.115753\n",
      "\n",
      "Test set: Average loss: 0.0765, Accuracy: 9739/10000 (97%) \n",
      "\n",
      "Train Epoch: 1 [14848/60000 (25)%] \t Loss: 0.063976\n",
      "Train Epoch: 1 [30208/60000 (50)%] \t Loss: 0.073350\n",
      "Train Epoch: 1 [45568/60000 (75)%] \t Loss: 0.151727\n",
      "\n",
      "Test set: Average loss: 0.0468, Accuracy: 9842/10000 (98%) \n",
      "\n",
      "Train Epoch: 2 [14848/60000 (25)%] \t Loss: 0.054588\n",
      "Train Epoch: 2 [30208/60000 (50)%] \t Loss: 0.047075\n",
      "Train Epoch: 2 [45568/60000 (75)%] \t Loss: 0.059006\n",
      "\n",
      "Test set: Average loss: 0.0451, Accuracy: 9857/10000 (99%) \n",
      "\n",
      "Train Epoch: 3 [14848/60000 (25)%] \t Loss: 0.020552\n",
      "Train Epoch: 3 [30208/60000 (50)%] \t Loss: 0.020728\n",
      "Train Epoch: 3 [45568/60000 (75)%] \t Loss: 0.047147\n",
      "\n",
      "Test set: Average loss: 0.0370, Accuracy: 9877/10000 (99%) \n",
      "\n",
      "Train Epoch: 4 [14848/60000 (25)%] \t Loss: 0.016404\n",
      "Train Epoch: 4 [30208/60000 (50)%] \t Loss: 0.068143\n",
      "Train Epoch: 4 [45568/60000 (75)%] \t Loss: 0.039820\n",
      "\n",
      "Test set: Average loss: 0.0352, Accuracy: 9882/10000 (99%) \n",
      "\n",
      "Train Epoch: 5 [14848/60000 (25)%] \t Loss: 0.025936\n",
      "Train Epoch: 5 [30208/60000 (50)%] \t Loss: 0.010987\n",
      "Train Epoch: 5 [45568/60000 (75)%] \t Loss: 0.009064\n",
      "\n",
      "Test set: Average loss: 0.0354, Accuracy: 9883/10000 (99%) \n",
      "\n",
      "Train Epoch: 6 [14848/60000 (25)%] \t Loss: 0.016676\n",
      "Train Epoch: 6 [30208/60000 (50)%] \t Loss: 0.017970\n",
      "Train Epoch: 6 [45568/60000 (75)%] \t Loss: 0.010014\n",
      "\n",
      "Test set: Average loss: 0.0284, Accuracy: 9896/10000 (99%) \n",
      "\n",
      "Train Epoch: 7 [14848/60000 (25)%] \t Loss: 0.010279\n",
      "Train Epoch: 7 [30208/60000 (50)%] \t Loss: 0.013812\n",
      "Train Epoch: 7 [45568/60000 (75)%] \t Loss: 0.012176\n",
      "\n",
      "Test set: Average loss: 0.0444, Accuracy: 9859/10000 (99%) \n",
      "\n",
      "Train Epoch: 8 [14848/60000 (25)%] \t Loss: 0.012159\n",
      "Train Epoch: 8 [30208/60000 (50)%] \t Loss: 0.009869\n",
      "Train Epoch: 8 [45568/60000 (75)%] \t Loss: 0.011225\n",
      "\n",
      "Test set: Average loss: 0.0374, Accuracy: 9888/10000 (99%) \n",
      "\n",
      "Train Epoch: 9 [14848/60000 (25)%] \t Loss: 0.004782\n",
      "Train Epoch: 9 [30208/60000 (50)%] \t Loss: 0.007760\n",
      "Train Epoch: 9 [45568/60000 (75)%] \t Loss: 0.018267\n",
      "\n",
      "Test set: Average loss: 0.0441, Accuracy: 9861/10000 (99%) \n",
      "\n",
      "Train Epoch: 10 [14848/60000 (25)%] \t Loss: 0.012370\n",
      "Train Epoch: 10 [30208/60000 (50)%] \t Loss: 0.009592\n",
      "Train Epoch: 10 [45568/60000 (75)%] \t Loss: 0.009334\n",
      "\n",
      "Test set: Average loss: 0.0343, Accuracy: 9880/10000 (99%) \n",
      "\n",
      "Train Epoch: 11 [14848/60000 (25)%] \t Loss: 0.016466\n",
      "Train Epoch: 11 [30208/60000 (50)%] \t Loss: 0.004146\n",
      "Train Epoch: 11 [45568/60000 (75)%] \t Loss: 0.013914\n",
      "\n",
      "Test set: Average loss: 0.0325, Accuracy: 9900/10000 (99%) \n",
      "\n",
      "Train Epoch: 12 [14848/60000 (25)%] \t Loss: 0.001570\n",
      "Train Epoch: 12 [30208/60000 (50)%] \t Loss: 0.000947\n",
      "Train Epoch: 12 [45568/60000 (75)%] \t Loss: 0.007050\n",
      "\n",
      "Test set: Average loss: 0.0366, Accuracy: 9883/10000 (99%) \n",
      "\n",
      "Train Epoch: 13 [14848/60000 (25)%] \t Loss: 0.005333\n",
      "Train Epoch: 13 [30208/60000 (50)%] \t Loss: 0.002802\n",
      "Train Epoch: 13 [45568/60000 (75)%] \t Loss: 0.002119\n",
      "\n",
      "Test set: Average loss: 0.0374, Accuracy: 9892/10000 (99%) \n",
      "\n",
      "Train Epoch: 14 [14848/60000 (25)%] \t Loss: 0.001622\n",
      "Train Epoch: 14 [30208/60000 (50)%] \t Loss: 0.001260\n",
      "Train Epoch: 14 [45568/60000 (75)%] \t Loss: 0.001335\n",
      "\n",
      "Test set: Average loss: 0.0367, Accuracy: 9902/10000 (99%) \n",
      "\n",
      "Train Epoch: 15 [14848/60000 (25)%] \t Loss: 0.001811\n",
      "Train Epoch: 15 [30208/60000 (50)%] \t Loss: 0.001790\n",
      "Train Epoch: 15 [45568/60000 (75)%] \t Loss: 0.001556\n",
      "\n",
      "Test set: Average loss: 0.0401, Accuracy: 9897/10000 (99%) \n",
      "\n",
      "Train Epoch: 16 [14848/60000 (25)%] \t Loss: 0.001557\n",
      "Train Epoch: 16 [30208/60000 (50)%] \t Loss: 0.000510\n",
      "Train Epoch: 16 [45568/60000 (75)%] \t Loss: 0.003440\n",
      "\n",
      "Test set: Average loss: 0.0376, Accuracy: 9900/10000 (99%) \n",
      "\n",
      "Train Epoch: 17 [14848/60000 (25)%] \t Loss: 0.003938\n",
      "Train Epoch: 17 [30208/60000 (50)%] \t Loss: 0.001442\n",
      "Train Epoch: 17 [45568/60000 (75)%] \t Loss: 0.003704\n",
      "\n",
      "Test set: Average loss: 0.0462, Accuracy: 9877/10000 (99%) \n",
      "\n",
      "Train Epoch: 18 [14848/60000 (25)%] \t Loss: 0.001844\n",
      "Train Epoch: 18 [30208/60000 (50)%] \t Loss: 0.003154\n",
      "Train Epoch: 18 [45568/60000 (75)%] \t Loss: 0.001628\n",
      "\n",
      "Test set: Average loss: 0.0427, Accuracy: 9890/10000 (99%) \n",
      "\n",
      "Train Epoch: 19 [14848/60000 (25)%] \t Loss: 0.005138\n",
      "Train Epoch: 19 [30208/60000 (50)%] \t Loss: 0.004494\n",
      "Train Epoch: 19 [45568/60000 (75)%] \t Loss: 0.001262\n",
      "\n",
      "Test set: Average loss: 0.0357, Accuracy: 9900/10000 (99%) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train(model, DEVICE, train_loader, optimizer, epoch)\n",
    "    test(model, DEVICE, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
