# 数据预处理   

在数据挖掘的过程中，数据预处理的工作占到了60%左右。    

主要内容包括了数据清洗，数据集成，数据变换和数据规约。   

## 数据清洗  

主要工作是删除原始数据集中的无关数据、重复数据、平滑噪声数据、筛选掉与挖掘主题无关的数据，处理缺失值、异常值等。   

### 缺失值的处理   

* 数据删除   
* 不处理  
* 数据插值   
    * 牛顿插值法   
    * 拉格朗日插值法   
        Scipy库中实现了拉格朗日插值法   
        使用拉格朗日插值法填补餐厅营业额中的缺失值    
        ```python
        import pandas as pd
        from scipy.interpolate import lagrange

        inputfile = '../data/catering_sale.xls'
        outputfile = '../tmp/sales.xls'

        data = pd.read_excel(inputfile)
        print(data.describe())
        # DataFrame.info()返回每个数据列的列名，以及每列非non的个数
        print(data.info())  # 这里发现有一天的销量数据丢失

        # 过滤异常值
        data[u'销量'][(data[u'销量'] < 400) | (data[u'销量'] > 5000)] = None


        # 定义插值函数
        def ployinterp_col(col, n, k=5):
            """
            :param col: col指被插值的列向量
            :param n: n指插值的位置
            :param k: k插值时考虑周围数据的量度
            :return:
            """
            # 获取用作插值运算的数据
            y = col[list(range(n - k, n)) + list(range(n + 1, n + 1 + k))]
            y = y[y.notnull()]  # 剔除空值
            return lagrange(y.index, list(y))(n)  # 插值并返回插值结果


        # 逐个元素判断是否需要进行插值
        for i in data.columns:
            for j in range(len(data)):
                if (data[i].isnull())[j]:  # 如果为空就需要进行插值
                    data[i][j] = ployinterp_col(data[i], j)

        data.to_excel(outputfile)
        ```  

## 数据变换  

### 简单函数变换   

* 简单的函数变换常用来将不具有正态分布的数据变换为具有正态分布的数据。    
* 某些简单的对数变换或者差分运算可以将非平稳序列转换为平稳序列   

* 使用对数变换还可以将动态范围压缩   


### 规范化   

数据规范化对于基于距离的挖掘算法尤为重要   

* 最小最大值规范化   

    $$x^* = \frac{x - min}{max - min}$$   

    规划范围为[0,1]   

    缺点是如果原数据中有某个特别大的数据，会导致规范化后的结果接近于0   

* 均值规范化   

    $$x^* = \frac{x - \overline{x}}{\sigma}$$   
    
    其中$\overline{x}$是原始数据的均值，$\sigma$为原始数据的标准差   
    经过该处理的数据均值为0，标准差为1    
    
* 小数定标规范化  

    移动数据的小数点位置，将属性值映射到[-1, 1]之间，移动的小数位数取决于原始数据绝对值的最大值    

    $$x^* = \frac{x}{10^k}$$    

* 分别使用上述三种规范化方法对一组数据进行操作       

    ```python
    import pandas as pd
    import numpy as np

    datafile = '../data/normalization_data.xls'
    data = pd.read_excel(datafile, header=None)

    # 最小最大值规范化
    print((data - data.min()) / (data.max() - data.min()))

    # 均值规范化
    print((data - data.mean()) / data.std())

    # 小数定标规范化
    print(data / 10 ** np.ceil(np.log10(data.abs().max())))
    ```   


### 连续属性离散化     

连续属性的离散化是在数据的取值范围内设定若干个离散的划分点，形成多个离散化的区间，然后用各个区间的标志或者整数值来代表每个子区间中的数据    

* 等宽法     

    将属性的取值范围划分成具有相同宽度的区间，区间的个数由数据本身的特点决定，或者人为指定   

* 等频法   

    将出现次数相同的记录放到一个区间中     

    缺点是对于离群点比较敏感，会导致最后属性值的分布不均匀,有的区间的数据很多，有的区间数据很少   

* 基于聚类分析的方法   

    使用K-Means算法进行聚，然后将聚类得到的簇进行处理.       

* 使用上述三种方式对一组数据进行离散化   

    ```python
    datafile = '../data/discretization_data.xls'

    data = pd.read_excel(datafile)
    data = data[u'肝气郁结证型系数'].copy()

    k = 4

    # 等宽离散化
    d1 = pd.cut(data, k, labels=range(k))  # 每个类别依次命名为0,1,2,3

    # 等频率离散化
    w = [1.0 * i / k for i in range(k + 1)]  # 先将0~1等分为五份
    w = data.describe(percentiles=w)[4:4 + k + 1]  # 计算分位数
    d2 = pd.cut(data, w, labels=range(k))

    # 聚类方法离散
    kmodel = KMeans(n_clusters=k)  # njobs为并行数
    kmodel.fit(data.values.reshape((len(data), 1)))  # 训练模型进行聚类

    c = pd.DataFrame(kmodel.cluster_centers_).sort_values(0)  # 输出聚类中心，并且排序
    w = c.rolling(2).mean().iloc[1:]  # 相邻两项求中点，作为边界点
    w = [0] + list(w[0]) + [data.max()]  # 把首尾边界点加上
    d3 = pd.cut(data, w, labels=range(k))


    # 绘图表示聚类结果
    def cluster_plot(d, k):
        plt.rcParams['font.sans-serif'] = ['SimHei']
        plt.rcParams['axes.unicode_minus'] = False

        plt.figure(figsize=(8, 3))
        for j in range(0, k):
            plt.plot(data[d == j], [j for i in d[d == j]], 'o')

        plt.ylim(-0.5, k - 0.5)
        return plt


    cluster_plot(d1, k).show()
    cluster_plot(d2, k).show()
    cluster_plot(d3, k).show()

    ```  

### 数据规约    

通过属性合并来创建新属性维数，或者直接删除不相关的属性来减少数据维数。    

* 主成分分析    

    一种用于连续属性的数据降维方法，为原始数据构造了一个新的正交变换，新空间的基底去除了原始基底的数据相关性，只需要少量的新变量就可以反映原始数据中的大部分变换。      

    ```python 
    from sklearn.decomposition import PCA

    inputfile = '../data/principal_component.xls'

    data = pd.read_excel(inputfile, header=None)

    pca = PCA()
    pca.fit(data)
    print(pca.components_)  # 模型的各个特征向量
    print(pca.explained_variance_ratio_)  # 各个成分各自的方差百分比
    ```  

    输出结果：   
    ```
    [[ 0.56788461  0.2280431   0.23281436  0.22427336  0.3358618   0.43679539
   0.03861081  0.46466998]
    [ 0.64801531  0.24732373 -0.17085432 -0.2089819  -0.36050922 -0.55908747
    0.00186891  0.05910423]
    [-0.45139763  0.23802089 -0.17685792 -0.11843804 -0.05173347 -0.20091919
    -0.00124421  0.80699041]
    [-0.19404741  0.9021939  -0.00730164 -0.01424541  0.03106289  0.12563004
    0.11152105 -0.3448924 ]
    [-0.06133747 -0.03383817  0.12652433  0.64325682 -0.3896425  -0.10681901
    0.63233277  0.04720838]
    [ 0.02579655 -0.06678747  0.12816343 -0.57023937 -0.52642373  0.52280144
    0.31167833  0.0754221 ]
    [-0.03800378  0.09520111  0.15593386  0.34300352 -0.56640021  0.18985251
    -0.69902952  0.04505823]
    [-0.10147399  0.03937889  0.91023327 -0.18760016  0.06193777 -0.34598258
    -0.02090066  0.02137393]]
    [7.74011263e-01 1.56949443e-01 4.27594216e-02 2.40659228e-02
    1.50278048e-03 4.10990447e-04 2.07718405e-04 9.24594471e-05]
    ```     

    上面的结果表示特征方程有7个特征根，对应7个单位特征向量以及各个成分的方差占比(贡献率)，其中方差百分比越大代表该向量的权重越大。     

    当选用前4个成分时，总贡献率就已经达到了97%，所以重新进行主成分分析，`n_components=3`      

    ```python
    # 重新进行主成分分析
    pca = PCA(3)
    pca.fit(data)
    low_data = pca.transform(data)  # 降低数据维度
    pd.DataFrame(low_data).to_excel(outputfile)  # 保存降维后的结果

    pca.inverse_transform(low_data)  # 复原数据
    print('pca_result : \n {}'.format(low_data))
    ```   

    输出：   
    结果是三列数据，代表三个特征向量     
    ```
    pca_result : 
    [[  8.19133694  16.90402785   3.90991029]
    [  0.28527403  -6.48074989  -4.62870368]
    [-23.70739074  -2.85245701  -0.4965231 ]
    [-14.43202637   2.29917325  -1.50272151]
    [  5.4304568   10.00704077   9.52086923]
    [ 24.15955898  -9.36428589   0.72657857]
    [ -3.66134607  -7.60198615  -2.36439873]
    [ 13.96761214  13.89123979  -6.44917778]
    [ 40.88093588 -13.25685287   4.16539368]
    [ -1.74887665  -4.23112299  -0.58980995]
    [-21.94321959  -2.36645883   1.33203832]
    [-36.70868069  -6.00536554   3.97183515]
    [  3.28750663   4.86380886   1.00424688]
    [  5.99885871   4.19398863  -8.59953736]]
    ```     

