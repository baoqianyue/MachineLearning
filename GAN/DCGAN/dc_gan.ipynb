{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练流程\n",
    "* 1.Data provider \n",
    "    * a.Image data\n",
    "    * b.random vector  \n",
    "* 2.Build Compute graph  \n",
    "    * a.generator \n",
    "    * b.discriminator \n",
    "    * c.DCGAN class \n",
    "        * connect g and d\n",
    "        * define loss\n",
    "        * define train_op\n",
    "* 3.training process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import logging\n",
    "from tensorflow import gfile\n",
    "import sys\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "from PIL import Image\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('../MNIST_data/', one_hot=True)# 加载数据\n",
    "output_dir = './local_run' # 输出文件夹路径\n",
    "if not gfile.Exists(output_dir):\n",
    "    gfile.MakeDirs(output_dir) # 创建输出文件夹\n",
    "\n",
    "def get_default_params():\n",
    "    return tf.contrib.training.HParams(\n",
    "        z_dim = 100, # 随机噪声数据维度\n",
    "        init_conv_size = 4, # 将输入噪声变化成feature_map时,其初始size为4x4\n",
    "        g_channels = [128, 64, 32, 1],# 生成器中各反卷积层的通道数目\n",
    "        d_channels = [32, 64, 128, 256], # 判别器中各卷积层的通道数据,各个卷积层size减半,然后通道数加半,然后pooling\n",
    "        batch_size = 128, # 每批数量\n",
    "        learning_rate = 0.002, \n",
    "        beta1 = 0.5, # adam参数\n",
    "        img_size = 32, # 生成图像为32x32\n",
    "    )\n",
    "hparams = get_default_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data provider\n",
    "class MnistData(object):\n",
    "    # z_dim随机向量的长度, img_size,将train中的img resize成img_size\n",
    "    def __init__(self, mnist_train, z_dim, img_size):\n",
    "        self._data = mnist_train\n",
    "        self._example_num = len(self._data)\n",
    "        # 随机向量使用正态分布生成\n",
    "        self._z_data = np.random.standard_normal((self._example_num, z_dim)) \n",
    "        # next batch 指示器\n",
    "        self._indicator = 0\n",
    "        # resize 图像大小,参数为要resize到的图像大小\n",
    "        self._resize_mnist_img(img_size)\n",
    "        # shuffle函数\n",
    "        self._random_shuffle()\n",
    "        \n",
    "    def _random_shuffle(self):\n",
    "        x = np.random.permutation(self._example_num)\n",
    "        # 随机化train和z\n",
    "        self._data = self._data[x]\n",
    "        self._z_data = self._z_data[x]  \n",
    "    \n",
    "    def _resize_mnist_img(self, img_size):\n",
    "        \"\"\"resize the img to target imgsize,first numpy to PIL img\"\"\"\n",
    "        # 1.在mnist中图像都被归一化,所以先将图像灰度级恢复到0~255,像素值类型为整形\n",
    "        data = np.asarray(self._data * 255, np.uint8)\n",
    "        # 2. 像素数据 [example_num, 784] -> [example_num, 28, 28]\n",
    "        data = data.reshape((self._example_num, 28, 28))\n",
    "        # 3.将每幅图像插值成img_size * img_size\n",
    "        new_data = []\n",
    "        for i in range(self._example_num):\n",
    "            img = data[i]\n",
    "            # 将numpy数据变成PIL对象\n",
    "            img = Image.fromarray(img)\n",
    "            img = img.resize((img_size, img_size))\n",
    "            # 再将PTL对象变成numpy对象\n",
    "            img = np.asarray(img)\n",
    "            # 判别器神经网络需要图像通道信息\n",
    "            img = img.reshape((img_size,img_size, 1))\n",
    "            new_data.append(img)\n",
    "        # 将resize后的全部图片转成numpy类型,new_data这里还是一个字典\n",
    "        new_data = np.asarray(new_data, dtype=np.float32)\n",
    "        # 将new_data归一化,这里将像素数据归一化到-1~1直接,为了配合generator,G中使用的是tanH\n",
    "        new_data = new_data / 127.5 - 1 \n",
    "        # self._data [example_num, img_size, img_size ,1]\n",
    "        self._data = new_data  \n",
    "        \n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"下一batch\"\"\"\n",
    "        end_indicator = self._indicator + batch_size  \n",
    "        if end_indicator > self._example_num: \n",
    "            # 如果end超出数据范围,直接shuffle,重新划分\n",
    "            self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = self._indicator + batch_size\n",
    "        assert end_indicator < self._example_num\n",
    "        \n",
    "        batch_data = self._data[self._indicator : end_indicator]\n",
    "        batch_z = self._z_data[self._indicator : end_indicator]\n",
    "        # 将当前indicator置为end_indicator,以便下一批次数据划分\n",
    "        self._indicator = end_indicator\n",
    "        return batch_data, batch_z \n",
    "\n",
    "mnist_data = MnistData(mnist.train.images, hparams.z_dim, hparams.img_size)\n",
    "batch_data, batch_z = mnist_data.next_batch(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generator \n",
    "def conv2d_transpose(inputs, out_channel, name, training, with_bn_relu=True):\n",
    "    \"\"\"\n",
    "    Wrapper of conv2d transpose\n",
    "    out_channel:每个反卷积层的通道数\n",
    "    name:该scope命名空间\n",
    "    training:在做卷积或者反卷积时都需要加一个bn操作,这里作为一个training开关\n",
    "    with_bn_relu=True:bn操作不作用于输出层和输入层,而且在生成器中其他层都使用relu激活,输出层使用tanH\n",
    "    所以这里默认为True,当到输出层时将该开关关闭\n",
    "    \"\"\"\n",
    "    conv2d_trans = tf.layers.conv2d_transpose(inputs,\n",
    "                                             out_channel,\n",
    "                                             [5,5],\n",
    "                                             strides=(2,2),\n",
    "                                             padding='SAME')\n",
    "    if with_bn_relu:\n",
    "        bn = tf.layers.batch_normalization(conv2d_trans, training=training)\n",
    "        return tf.nn.relu(bn)\n",
    "    else:\n",
    "        return conv2d_trans  \n",
    "    \n",
    "# discriminator \n",
    "def conv2d(inputs, out_channel, name, training):\n",
    "    \"\"\"\n",
    "    Wrapper of conv2d\n",
    "    out_channel:每个卷积层的通道数\n",
    "    name:该scope命名空间\n",
    "    training:卷积层后要做bn操作,这里给一个training开关\n",
    "    \"\"\"\n",
    "    # 判别器中使用leaky_relu,斜率为0.2\n",
    "    def leaky_relu(x, leak=0.2, name=' '):\n",
    "        return tf.maximum(x, x * leak, name=name)\n",
    "    \n",
    "    # 加入卷积层\n",
    "    with tf.variable_scope(name):\n",
    "        conv2d_output = tf.layers.conv2d(inputs,\n",
    "                                        out_channel,\n",
    "                                        [5,5],\n",
    "                                        strides=(2,2),# 这里步长为2,每次卷积输出是输入size的1/2\n",
    "                                        padding='SAME')\n",
    "        # 对卷积输出做bn操作\n",
    "        bn = tf.layers.batch_normalization(conv2d_output, training=training)\n",
    "        # 非线性变换\n",
    "        return leaky_relu(bn, name='outputs')\n",
    "        \n",
    "class Generator(object):\n",
    "    def __init__(self, channels, init_conv_size):\n",
    "        \"\"\"\n",
    "        channels:各个反卷积层中的通道数\n",
    "        \"\"\"\n",
    "        self._channels = channels\n",
    "        self._init_conv_size = init_conv_size\n",
    "        # reuse 开关,在第一次构建计算图完成后,就将该重用标志打开\n",
    "        self._reuse = False\n",
    "        \n",
    "    def __call__(self, inputs, training):\n",
    "        # 首先将输入数据转成tensor类型\n",
    "        inputs = tf.convert_to_tensor(inputs)\n",
    "        with tf.variable_scope('generator', reuse=self._reuse):\n",
    "            \"\"\"\n",
    "            1.先对随机向量做一个fc层,参数W矩阵的size由init_conv_size给出\n",
    "            fc层仍是一个向量 \n",
    "            random_vector -> fc (size=[self.channels[0] * init_conv_size**2])\n",
    "            2.对fc层进行reshape,将其变换为一个三维矩阵形式(图像形式)\n",
    "            \"\"\"\n",
    "            with tf.variable_scope('inputs_conv'):\n",
    "                # fc层的size是self.channels[0] * init_conv_size ** 2\n",
    "                fc = tf.layers.dense(inputs, \n",
    "                                    self._channels[0] * self._init_conv_size * self._init_conv_size)\n",
    "                # 对fc层进行reshape成一个图像格式\n",
    "                # 可以看做是一个卷积层输出,shape参数中第一个-1是next_batch的大小\n",
    "                conv0 = tf.reshape(fc, [-1,\n",
    "                                       self._init_conv_size,\n",
    "                                       self._init_conv_size,\n",
    "                                       self._channels[0]])\n",
    "                # 对该图像bn操作\n",
    "                bn0 = tf.layers.batch_normalization(conv0, training=training)\n",
    "                # 非线性变换\n",
    "                relu0 = tf.nn.relu(bn0)\n",
    "                \n",
    "            # 进行反卷积操作\n",
    "            deconv_inputs = relu0 \n",
    "            # 循环实现多层反卷积,这里从1开始循环,因为真正反卷积的channel是从self._channels[1]开始的\n",
    "            for i in range(1, len(self._channels)):\n",
    "                # 首先要判断是否是生成器的输出层(是否是反卷积的最后一层),该层无需进行bn和relu操作\n",
    "                with_bn_relu = (i != len(self._channels) - 1)  \n",
    "                # 直接调用上面封装好的反卷积函数,将反卷积的输出保存在deconv_inputs中,方便下一步生成图像\n",
    "                deconv_inputs = conv2d_transpose(\n",
    "                    deconv_inputs,\n",
    "                    self._channels[i],\n",
    "                    'deconv-%d' % i,\n",
    "                    training,\n",
    "                    with_bn_relu)\n",
    "            # 对最后一个反卷积输出层使用tanH激活,生成图像,图像像素值范围为[-1,1]\n",
    "            img_inputs = deconv_inputs\n",
    "            with tf.variable_scope('generate_imgs'):\n",
    "                imgs = tf.tanh(img_inputs, name='imgs')\n",
    "        # 在第一次计算图构建完成后,将resue打开\n",
    "        self._reuse = True\n",
    "        \n",
    "        # 在GAN中,生成器和判别器是分开训练的,对应两个,所以两个网络中的参数也要分开保存 \n",
    "        self.variables = tf.get_collection(\n",
    "            tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "            scope='generator')\n",
    "        # 再将生成的图像返回\n",
    "        return imgs  \n",
    "    \n",
    "class Discriminator(object):\n",
    "    def __init__(self, channels):\n",
    "        self._channels = channels\n",
    "        self._reuse = False\n",
    "        \n",
    "    def __call__(self, inputs, training):\n",
    "        # 首先将输入数据转成tensor类型,然后再进行卷积操作\n",
    "        inputs = tf.convert_to_tensor(inputs, dtype=tf.float32)\n",
    "        conv_inputs = inputs\n",
    "        with tf.variable_scope('discriminator', reuse=self._reuse):\n",
    "            \"\"\"\n",
    "            1.直接对输入图像进行卷积操作\n",
    "            2.将卷积输出flatten\n",
    "            3.对flatten后的数据生成一个fc层,输出shape为2,值为0或1,表示当前判断结果\n",
    "            \"\"\"\n",
    "            for i in range(len(self._channels)):\n",
    "                conv_inputs = conv2d(conv_inputs,\n",
    "                                    self._channels[i],\n",
    "                                    'conv-%d' % i,\n",
    "                                    training)\n",
    "            # 全连接层\n",
    "            fc_inputs = conv_inputs\n",
    "            with tf.variable_scope('fc'):\n",
    "                flatten = tf.layers.flatten(fc_inputs)\n",
    "                # 全连接层输出种类为2种,分别代表判别为真实图像和生成图像\n",
    "                logits = tf.layers.dense(flatten, 2, name='logits')\n",
    "        # 网络构建后,将reuse开关打开\n",
    "        self._reuse = True\n",
    "        # 将判别器中的参数保存\n",
    "        self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='discriminator')\n",
    "        # 将判别器判别结果返回\n",
    "        return logits\n",
    "\n",
    "# connect generator and discriminator\n",
    "class DCGAN(object):\n",
    "    def __init__(self, hps):\n",
    "        g_channels = hps.g_channels # 生成器中各反卷积层数\n",
    "        d_channels = hps.d_channels # 判别器中各卷积层数\n",
    "        \n",
    "        self._batch_size = hps.batch_size \n",
    "        self._init_conv_size = hps.init_conv_size\n",
    "        self._z_dim = hps.z_dim\n",
    "        self._img_size = hps.img_size\n",
    "        \n",
    "        # 创建G和D的对象\n",
    "        self._generator = Generator(g_channels, self._init_conv_size)\n",
    "        self._discriminator = Discriminator(d_channels)\n",
    "        \n",
    "    def build(self):\n",
    "        \"\"\"\n",
    "        构建计算图\n",
    "        \"\"\"\n",
    "        # 先定义两个placeholder\n",
    "        # 随机向量的placeholder\n",
    "        self._z_placeholder = tf.placeholder(tf.float32, (self._batch_size, self._z_dim))\n",
    "        # 真实图像的placeholder\n",
    "        self._img_placeholder = tf.placeholder(tf.float32, \n",
    "                                               (self._batch_size, self._img_size, self._img_size, 1)) \n",
    "        # 得到生成图像,将随机向量的placeholder作为输入\n",
    "        generated_imgs = self._generator(self._z_placeholder, training=True)\n",
    "        \n",
    "        # 计算真实图像和生成图像各自的判别结果\n",
    "        fake_img_logits = self._discriminator(generated_imgs, training=True)\n",
    "        real_img_logits = self._discriminator(self._img_placeholder, training=True)\n",
    "        \n",
    "        # 分别定义两个网络的损失函数\n",
    "        # 对于生成器来说,希望将其生成的图像输入到判别器中,判别结果为1\n",
    "        loss_on_fake_to_real = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                # 这里的labels对应每个图像的真是标签都是1,所以可以直接使用sparse的loss\n",
    "                labels=tf.ones([self._batch_size], dtype=tf.int64),\n",
    "                logits=fake_img_logits))\n",
    "        # 对于判别器来说,希望判别输入图像为真实图像时判别结果为1\n",
    "        # 希望判别输入图像为生成的图像时判别结果为0\n",
    "        loss_on_real_to_real = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels=tf.ones([self._batch_size], dtype=tf.int64),\n",
    "                logits=real_img_logits))\n",
    "        loss_on_fake_to_fake = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels=tf.zeros([self._batch_size], dtype=tf.int64),\n",
    "                logits=fake_img_logits))\n",
    "        \n",
    "        # 计算两个网络的总loss\n",
    "        # G\n",
    "        tf.add_to_collection('g_losses', loss_on_fake_to_real)\n",
    "        # D\n",
    "        tf.add_to_collection('d_losses', loss_on_real_to_real)\n",
    "        tf.add_to_collection('d_losses', loss_on_fake_to_fake)\n",
    "        \n",
    "        # total loss\n",
    "        loss = {\n",
    "            'g' : tf.add_n(tf.get_collection('g_losses'),\n",
    "                          name='total_g_loss'),\n",
    "            'd' : tf.add_n(tf.get_collection('d_losses'),\n",
    "                          name='total_d_loss')\n",
    "        }\n",
    "        \n",
    "        return (self._z_placeholder, \n",
    "                self._img_placeholder, \n",
    "                generated_imgs, \n",
    "                loss)\n",
    "    \n",
    "    def build_train_op(self, losses, learning_rate, beta1):\n",
    "        \"\"\"\n",
    "        该函数必须在build构建计算图之后执行\n",
    "        beta1是adamOptimizer的超参数\n",
    "        \"\"\"\n",
    "        # 两个网络分开训练,这里分开定义两个opt\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                      beta1=beta1)\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                      beta1=beta1)\n",
    "        \n",
    "        # 传入各自的loss和参数变量\n",
    "        g_opt_op = g_opt.minimize(\n",
    "            losses['g'], var_list=self._generator.variables)\n",
    "        d_opt_op = d_opt.minimize(\n",
    "            losses['d'], var_list=self._discriminator.variables)\n",
    "        \n",
    "        # 为了实现G和D是交叉训练的,即训练一次生成器就训练一次判别器\n",
    "        with tf.control_dependencies([g_opt_op, d_opt_op]):\n",
    "            return tf.no_op(name='train')\n",
    "        \n",
    "dcgan = DCGAN(hparams)\n",
    "z_placeholder, img_placeholder, generated_imgs, losses = dcgan.build()\n",
    "train_op = dcgan.build_train_op(losses, hparams.learning_rate, hparams.beta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:step:  499, g_loss: 0.869, d_loss: 0.786\n",
      "INFO:tensorflow:step:  999, g_loss: 0.766, d_loss: 1.005\n",
      "INFO:tensorflow:step: 1499, g_loss: 1.590, d_loss: 0.669\n",
      "INFO:tensorflow:step: 1999, g_loss: 1.333, d_loss: 0.797\n",
      "INFO:tensorflow:step: 2499, g_loss: 2.912, d_loss: 0.499\n",
      "INFO:tensorflow:step: 2999, g_loss: 1.642, d_loss: 0.377\n",
      "INFO:tensorflow:step: 3499, g_loss: 3.215, d_loss: 0.607\n",
      "INFO:tensorflow:step: 3999, g_loss: 3.222, d_loss: 0.471\n",
      "INFO:tensorflow:step: 4499, g_loss: 3.852, d_loss: 0.086\n",
      "INFO:tensorflow:step: 4999, g_loss: 1.802, d_loss: 0.434\n",
      "INFO:tensorflow:step: 5499, g_loss: 3.792, d_loss: 0.047\n",
      "INFO:tensorflow:step: 5999, g_loss: 5.662, d_loss: 0.200\n",
      "INFO:tensorflow:step: 6499, g_loss: 4.529, d_loss: 0.363\n",
      "INFO:tensorflow:step: 6999, g_loss: 3.421, d_loss: 0.268\n",
      "INFO:tensorflow:step: 7499, g_loss: 4.430, d_loss: 0.027\n",
      "INFO:tensorflow:step: 7999, g_loss: 4.151, d_loss: 0.049\n",
      "INFO:tensorflow:step: 8499, g_loss: 5.659, d_loss: 0.037\n",
      "INFO:tensorflow:step: 8999, g_loss: 4.398, d_loss: 0.060\n",
      "INFO:tensorflow:step: 9499, g_loss: 2.429, d_loss: 0.171\n",
      "INFO:tensorflow:step: 9999, g_loss: 2.728, d_loss: 0.123\n"
     ]
    }
   ],
   "source": [
    "# training process\n",
    "def combine_imgs(batch_imgs, img_size, rows=8, cols=16):\n",
    "    \"\"\"\n",
    "    把一个batch的小图像合并成一个大图,方便显示\n",
    "    \"\"\"\n",
    "    # batch_imgs :[batch_size, img_size, img_size, 1]\n",
    "    result_big_img = []\n",
    "    for i in range(rows):\n",
    "        row_imgs = [] # 每一行的合并图像\n",
    "        for j in range(cols):\n",
    "            img = batch_imgs[i * cols + j] # 获取到某个图像\n",
    "            # 这里有可能是train_img,原始size为28x28,这里将他们统一成32x32\n",
    "            img = img.reshape((img_size, img_size))\n",
    "            # 生成的图像像素值是-1到1,这里将像素值恢复到0~255\n",
    "            img = (img + 1) * 127.5\n",
    "            row_imgs.append(img)\n",
    "        # 这里row_imgs里面都是单个的图像,在横向将他们合并\n",
    "        row_imgs = np.hstack(row_imgs)\n",
    "        # 将上面合并后的一行图像放入结果\n",
    "        result_big_img.append(row_imgs)\n",
    "    # 当前result_big_img中是8行图像,将他们合并\n",
    "    result_big_img = np.vstack(result_big_img)\n",
    "    # 为了显示图像,将像素值类型改为整形\n",
    "    result_big_img = np.asarray(result_big_img, np.uint8)\n",
    "    # 转成PIL类型\n",
    "    result_big_img = Image.fromarray(result_big_img)\n",
    "    return result_big_img\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "# 迭代10000次\n",
    "train_steps = 10000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for step in range(train_steps):\n",
    "        # 每次训练先去取训练数据\n",
    "        batch_imgs, batch_z = mnist_data.next_batch(hparams.batch_size)\n",
    "        # 每次迭代需要的元素,op,两个网络的loss\n",
    "        fetches = [train_op, losses['g'], losses['d']]\n",
    "        # 每迭代50次保存一次生成图像\n",
    "        should_sample = (step + 1) % 50 == 0\n",
    "        # 每500次打印一次训练信息\n",
    "        is_show_message = (step + 1) % 500 == 0\n",
    "        if should_sample:\n",
    "            fetches += [generated_imgs] # 每五十次将生成的图像显示并保存一下\n",
    "        output_values = sess.run(fetches, feed_dict={\n",
    "            z_placeholder: batch_z,\n",
    "            img_placeholder: batch_imgs\n",
    "        })\n",
    "        # 读取训练信息\n",
    "        _, g_loss_val, d_loss_val = output_values[0 : 3]\n",
    "        if is_show_message:\n",
    "            logging.info('step: %4d, g_loss: %4.3f, d_loss: %4.3f'\n",
    "                    % (step, g_loss_val, d_loss_val))\n",
    "        \n",
    "        # 将每50次后的结果保存一次\n",
    "        if should_sample:\n",
    "            gen_imgs_val = output_values[3] # 生成的图像\n",
    "            # 生成图像和原始图像的路径\n",
    "            gen_img_path = os.path.join(output_dir, '%05d-gen.jpg' % (step + 1))\n",
    "            raw_img_path = os.path.join(output_dir, '%05d-raw.jpg' % (step + 1))\n",
    "            # 将生成的图像和一个batchSize的原始图像合并\n",
    "            gen_img = combine_imgs(gen_imgs_val, hparams.img_size)\n",
    "            raw_img = combine_imgs(batch_imgs, hparams.img_size)\n",
    "            # 保存图像\n",
    "            gen_img.save(gen_img_path)\n",
    "            raw_img.save(raw_img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练结果\n",
    "* 训练500次后生成的图像 \n",
    "    * 训练图像 \n",
    "        ![img](./local_run/00500-raw.jpg)  \n",
    "    * 生成图像   \n",
    "        ![img](./local_run/00500-gen.jpg)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 训练5000次的生成图像   \n",
    "    ![img](./local_run/05000-gen.jpg)\n",
    "* 训练9000次的生成图像   \n",
    "    ![img](./local_run/09000-gen.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
